{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaUM8kP2X1ewAf26BwfHT9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/angelinetipa/MyDataCampJourney/blob/master/Data_Manipulation_with_pandas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transforming DataFrames**\n",
        "- master the pandas basics. Learn how to inspect DataFrames and perform fundamental manipulations, including sorting rows, subsetting, and adding new columns."
      ],
      "metadata": {
        "id": "FhRlMdjc2SaC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Pandas**\n",
        "- python package for data manipulation\n",
        "- used for data visualizations\n",
        "- built on top of 2 python packages, **NumPy** and **Matplotlib**\n",
        "  - **Numpy** - multidimensional array objects for easy data manipulation that pandas uses to store data\n",
        "  - **Matplotlib** - has powerful data visualization capabilities that pandas takes advantage of.\n",
        "- rectangular data is represented as a **DataFrame** object\n",
        "  - The **columns attribute** contains column names, and the **index attribute** contains row numbers or row names.\n",
        "1. Dataframes - chapter 1\n",
        "  - core of pandas\n",
        "\n",
        "2. Aggregating Data - chapter 2\n",
        "  - to gather insights\n",
        "\n",
        "3. Slicing and Indexing Data - chapter 3\n",
        "  - susbset dataframes\n",
        "\n",
        "4. Visualize, deal with missing and read data - chapter 4\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PULSzCf5F-G7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        ".head() returns the first few rows (the “head” of the DataFrame).\n",
        ".info() shows information on each of the columns, such as the data type and number of missing values.\n",
        ".shape returns the number of rows and columns of the DataFrame.\n",
        ".describe() calculates a few summary statistics for each column.\n",
        "```"
      ],
      "metadata": {
        "id": "c4NeLqpsIjkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand DataFrame objects, it's useful to know that they consist of three components, stored as attributes:\n",
        "```\n",
        ".values: A two-dimensional NumPy array of values.\n",
        ".columns: An index of columns: the column names.\n",
        ".index: An index for the rows: either row numbers or row names.\n",
        "```\n"
      ],
      "metadata": {
        "id": "1YaXJuVsJJeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SORTING AND SUBSETTING**\n",
        "- The **outer square brackets** are responsible for subsetting the DataFrame\n",
        "- **inner square brackets** are creating a list of column names to subset.\n",
        "- sort the rows by passing a column name to `.sort_values().`\n",
        "```\n",
        "# Sort homelessness by region, then descending family members\n",
        "homelessness_reg_fam = homelessness.sort_values([\"region\", \"family_members\"], ascending=[True,False])\n",
        "# Print the top few rows\n",
        "print(homelessness_reg_fam.head())\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9npWILrHKHLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NEW COLUMNS**\n",
        "**Types of data manipulation:**\n",
        "* sorting rows,\n",
        "* subsetting columns\n",
        "* subsetting rows\n",
        "* adding new columns."
      ],
      "metadata": {
        "id": "J9KPDuY0c-DY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIL4OR96Fo18"
      },
      "outputs": [],
      "source": [
        "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
        "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"]/ homelessness[\"state_pop\"]\n",
        "\n",
        "# Subset rows for indiv_per_10k greater than 20\n",
        "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
        "\n",
        "# Sort high_homelessness by descending indiv_per_10k\n",
        "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
        "\n",
        "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
        "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
        "\n",
        "# See the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Aggregating DataFrames**\n",
        "- calculate summary statistics on DataFrame columns, and master grouped summary statistics and pivot tables."
      ],
      "metadata": {
        "id": "o1qKWSWE2jm0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **SUMMARY STATISTICS**\n",
        "- numbers that summarize and tell you about your dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "zkDXQqihhVJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ffWkgUYShZiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the head of the sales DataFrame\n",
        "print(sales.head())\n",
        "\n",
        "# Print the info about the sales DataFrame\n",
        "print(sales.info())\n",
        "\n",
        "# Print the mean of weekly_sales\n",
        "print(sales[\"weekly_sales\"].mean())\n",
        "\n",
        "# Print the median of weekly_sales\n",
        "print(sales[\"weekly_sales\"].median())\n",
        "\n",
        "# Print the maximum of the date column\n",
        "print(sales[\"date\"].maximum())"
      ],
      "metadata": {
        "id": "466KPJOOixtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A custom IQR function\n",
        "def iqr(column):\n",
        "    return column.quantile(0.75) - column.quantile(0.25)\n",
        "\n",
        "# Print IQR of the temperature_c column\n",
        "print(sales[\"temperature_c\"].agg(iqr))"
      ],
      "metadata": {
        "id": "DxioErN5jjgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`.agg()` method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient."
      ],
      "metadata": {
        "id": "fRhWoicKjnWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import NumPy and create custom IQR function\n",
        "import numpy as np\n",
        "def iqr(column):\n",
        "    return column.quantile(0.75) - column.quantile(0.25)\n",
        "\n",
        "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
        "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
      ],
      "metadata": {
        "id": "5SOpq59qkZVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cumulative Statistics**"
      ],
      "metadata": {
        "id": "H_z7fX-Xl1nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort sales_1_1 by date\n",
        "sales_1_1 = sales_1_1.sort_values(\"date\", ascending=True)\n",
        "\n",
        "# Get the cumulative sum of weekly_sales, add as cum_weekly_sales col\n",
        "sales_1_1[\"cum_weekly_sales\"] = sales_1_1[\"weekly_sales\"].cumsum()\n",
        "\n",
        "# Get the cumulative max of weekly_sales, add as cum_max_sales col\n",
        "sales_1_1[\"cum_max_sales\"]=sales_1_1[\"weekly_sales\"].cummax()\n",
        "\n",
        "# See the columns you calculated\n",
        "print(sales_1_1[[\"date\", \"weekly_sales\", \"cum_weekly_sales\", \"cum_max_sales\"]])"
      ],
      "metadata": {
        "id": "vmEkMyMVl7RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop duplicate store/department combinations\n",
        "store_depts = sales.drop_duplicates(subset=['store', 'department'])\n",
        "print(store_depts.head())"
      ],
      "metadata": {
        "id": "RQzYqrvuvX7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of stores of each type\n",
        "store_types[\"type\"].value_counts()\n",
        "print(store_counts)\n",
        "\n",
        "# Get the proportion of stores of each type\n",
        "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
        "print(store_props)\n",
        "\n",
        "# Count the number of stores for each department and sort\n",
        "dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n",
        "print(dept_counts_sorted)\n",
        "\n",
        "# Get the proportion of stores in each department and sort\n",
        "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
        "print(dept_props_sorted)"
      ],
      "metadata": {
        "id": "QyBH8Lv3xjkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate total weekly sales for all stores\n",
        "sales_all = sales[\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type A stores and calculate total weekly sales\n",
        "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type B stores and calculate total weekly sales\n",
        "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Subset for type C stores and calculate total weekly sales\n",
        "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
        "\n",
        "# Calculate proportion of sales for each store type\n",
        "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
        "print(sales_propn_by_type)\n"
      ],
      "metadata": {
        "id": "p2RMhzdYzStc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
        "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
        "\n",
        "# Print sales_stats\n",
        "print(sales_stats)\n",
        "\n",
        "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
        "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\", \"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
        "\n",
        "# Print unemp_fuel_stats\n",
        "print(unemp_fuel_stats)\n"
      ],
      "metadata": {
        "id": "S3h5yHeZ2k9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pivot Tables**\n",
        "- way of calculating grouped summary statistics\n",
        "- are the standard way of aggregating data in spreadsheets."
      ],
      "metadata": {
        "id": "qlaLorwI20DR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", aggfunc=\"mean\", fill_value=0))\n",
        "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
        "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", aggfunc=\"mean\", fill_value=0, margins=True))"
      ],
      "metadata": {
        "id": "fo19EvtL8NEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Slicing and Indexing DataFrames**\n",
        "- Indexes are supercharged row and column names. Learn how they can be combined with slicing for powerful DataFrame subsetting."
      ],
      "metadata": {
        "id": "_7Y4a8MJ29BX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting and removing indexes\n"
      ],
      "metadata": {
        "id": "x8Hc216f3Fph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at temperatures\n",
        "print(temperatures)\n",
        "\n",
        "# Set the index of temperatures to city\n",
        "temperatures_ind = temperatures.set_index(\"city\")\n",
        "\n",
        "# Look at temperatures_ind\n",
        "print(temperatures_ind)\n",
        "\n",
        "# Reset the temperatures_ind index, keeping its contents\n",
        "print(temperatures_ind.reset_index())\n",
        "\n",
        "# Reset the temperatures_ind index, dropping its contents\n",
        "print(temperatures_ind.reset_index(drop=True))"
      ],
      "metadata": {
        "id": "BlcMeB8sB8yr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsetting with `.loc()`"
      ],
      "metadata": {
        "id": "-P0myyiDD8Ei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a list of cities to subset on\n",
        "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
        "\n",
        "# Subset temperatures using square brackets\n",
        "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
        "\n",
        "# Subset temperatures_ind using .loc[]\n",
        "print(temperatures_ind.loc[cities])"
      ],
      "metadata": {
        "id": "zM-FimMjEAfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting multi-level indexes"
      ],
      "metadata": {
        "id": "eaU2o9uQFYxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index temperatures by country & city\n",
        "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
        "\n",
        "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
        "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\", \"Pakistan\", \"Lahore\")]\n",
        "\n",
        "# Subset for rows to keep\n",
        "print(temperatures_ind.loc[rows_to_keep])"
      ],
      "metadata": {
        "id": "lKhiXg4RFc4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sorting by index values"
      ],
      "metadata": {
        "id": "3tra_qg-HbFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort temperatures_ind by index values\n",
        "print(temperatures_ind.sort_index())\n",
        "\n",
        "# Sort temperatures_ind by index values at the city level\n",
        "print(temperatures_ind.sort_index(level= 'city'))\n",
        "\n",
        "# Sort temperatures_ind by country then descending city\n",
        "print(temperatures_ind.sort_values(by=['country', 'city'], ascending=[True, False]))"
      ],
      "metadata": {
        "id": "SSjb3hNZHeGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- You can only slice an index if the index is sorted (using `.sort_index()`).\n",
        "- To slice at the outer level, `first` and `last` can be strings.\n",
        "- To slice at inner levels, `first` and `last` should be tuples.\n",
        "- If you pass a single slice to `.loc[]`, it will slice the rows."
      ],
      "metadata": {
        "id": "80cFtIvyI6RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing Index Values"
      ],
      "metadata": {
        "id": "ARGQZl90KTtw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the index of temperatures_ind\n",
        "temperatures_srt = temperatures_ind.sort_index()\n",
        "\n",
        "# Subset rows from Pakistan to Russia\n",
        "print(temperatures_srt.loc['Pakistan':'Russia'])\n",
        "\n",
        "# Try to subset rows from Lahore to Moscow\n",
        "print(temperatures_srt.loc['Lahore':'Moscow'])\n",
        "\n",
        "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
        "print(temperatures_srt.loc[('Pakistan', 'Lahore'):('Russia', 'Moscow')])"
      ],
      "metadata": {
        "id": "uO-pbLpSKWtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing in both directions"
      ],
      "metadata": {
        "id": "buFmty9ALVVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
        "print(temperatures_srt.loc[('India', 'Hyderabad'):('Iraq', 'Baghdad')])\n",
        "\n",
        "# Subset columns from date to avg_temp_c\n",
        "print(temperatures_srt.loc[:,'date':'avg_temp_c'])\n",
        "\n",
        "# Subset in both directions at once\n",
        "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])"
      ],
      "metadata": {
        "id": "-4cWePMuLqwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing time series"
      ],
      "metadata": {
        "id": "LOOPtSZ1Lxy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
        "temperatures_bool = temperatures[(temperatures['date'] >= '2010-01-01') & (temperatures['date'] <= '2011-12-31')]\n",
        "print(temperatures_bool)\n",
        "\n",
        "# Set date as the index and sort the index\n",
        "temperatures_ind = temperatures.set_index('date').sort_index()\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
        "print(temperatures_ind.loc['2010':'2011'])\n",
        "\n",
        "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
        "print(temperatures_ind.loc['2010-08':'2011-02'])\n"
      ],
      "metadata": {
        "id": "NXjoeoBsMVjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsetting by row/column number"
      ],
      "metadata": {
        "id": "WyhYU6j19lK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get 23rd row, 2nd column (index 22, 1)\n",
        "print(temperatures.iloc[22, 1])\n",
        "\n",
        "# Use slicing to get the first 5 rows\n",
        "print(temperatures.iloc[:5])\n",
        "\n",
        "# Use slicing to get columns 3 to 4\n",
        "print(temperatures.iloc[:, 2:4])\n",
        "\n",
        "# Use slicing in both directions at once (first 5 rows, columns 3 to 4)\n",
        "print(temperatures.iloc[:5, 2:4])\n"
      ],
      "metadata": {
        "id": "hvSjYZY99l2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pivot temperature by city and year"
      ],
      "metadata": {
        "id": "aZYiO2vY_CFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a year column to temperatures\n",
        "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
        "\n",
        "# Pivot avg_temp_c by country and city vs year\n",
        "temp_by_country_city_vs_year = temperatures.pivot_table(\n",
        "    values=\"avg_temp_c\",\n",
        "    index=[\"country\", \"city\"],\n",
        "    columns=\"year\"\n",
        ")\n",
        "\n",
        "# See the result\n",
        "print(temp_by_country_city_vs_year)\n"
      ],
      "metadata": {
        "id": "Ciu6duW-_Cqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subsetting pivot tables"
      ],
      "metadata": {
        "id": "YTVdvLU5_XmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset for Egypt to India\n",
        "temp_by_country_city_vs_year.loc['Egypt':'India']\n",
        "\n",
        "# Subset for Egypt, Cairo to India, Delhi\n",
        "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi')]\n",
        "\n",
        "\n",
        "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
        "temp_by_country_city_vs_year.loc[('Egypt','Cairo'):('India','Delhi'), '2005':'2010']\n"
      ],
      "metadata": {
        "id": "qt4sHeM0_ZyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating on pivot table"
      ],
      "metadata": {
        "id": "6Eexle3IAiIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the worldwide mean temp by year\n",
        "mean_temp_by_year = temp_by_country_city_vs_year.mean(axis=0)\n",
        "\n",
        "# Filter for the year that had the highest mean temp\n",
        "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
        "\n",
        "# Get the mean temp by city\n",
        "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=1)\n",
        "\n",
        "# Filter for the city that had the lowest mean temp\n",
        "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])\n"
      ],
      "metadata": {
        "id": "OCzt_vy0Ak3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visualizing your data**\n",
        "- Learn to visualize the contents of your DataFrames, handle missing data values, and import data from and export data to CSV files."
      ],
      "metadata": {
        "id": "r4461lK5Aoeu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BAR PLOTS**\n",
        "- great for revealing relationships between categorical (size) and numeric (number sold) variables.\n",
        "\n",
        "**LINE PLOTS**\n",
        "- designed to visualize the relationship between two numeric variables, where each data values is connected to the next one.\n",
        "- useful for visualizing the change in a number over time\n",
        "\n",
        "**SCATTER PLOTS**\n",
        "- ideal for visualizing relationships between numerical variables.\n"
      ],
      "metadata": {
        "id": "6FIciOwWIa4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding missing values"
      ],
      "metadata": {
        "id": "xkZ6rLosSrLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import matplotlib.pyplot with alias plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check individual values for missing values\n",
        "print(avocados_2016.isna())\n",
        "\n",
        "# Check each column for missing values\n",
        "print(avocados_2016.isna().any())\n",
        "\n",
        "# Bar plot of missing values by variable\n",
        "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
        "\n",
        "# Show plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "e3P8jryrStUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing missing values"
      ],
      "metadata": {
        "id": "REOSJs62Tsqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove rows with missing values\n",
        "avocados_complete = avocados_2016.dropna()\n",
        "\n",
        "# Check if any columns contain missing values\n",
        "print(avocados_complete.isna().any())"
      ],
      "metadata": {
        "id": "ycAYzeMET41A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing missing values"
      ],
      "metadata": {
        "id": "mznRlLjaVimU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From previous step\n",
        "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
        "avocados_2016[cols_with_missing].hist()\n",
        "plt.show()\n",
        "\n",
        "# Fill in missing values with 0\n",
        "avocados_filled = avocados_2016.fillna(0)\n",
        "\n",
        "# Create histograms of the filled columns\n",
        "avocados_filled[cols_with_missing].hist()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dsvP0fC-VlIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CREATING DATAFRAMES**\n",
        "\n",
        "**Dictionaries**\n",
        "- way of storing data in python.\n",
        "- it holds a set of key-value pairs"
      ],
      "metadata": {
        "id": "ZhyS3zTOVxqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "List of Dictionaries"
      ],
      "metadata": {
        "id": "N7KUiQ-HYMRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of dictionaries with new data\n",
        "avocados_list = [\n",
        "    {'date': \"2019-11-03\", 'small_sold': 10376832, 'large_sold': 7835071},\n",
        "    {'date': \"2019-11-10\", 'small_sold': 10717154, 'large_sold': 8561348},\n",
        "]\n",
        "\n",
        "# Convert list into DataFrame\n",
        "avocados_2019 = pd.DataFrame(avocados_list)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(avocados_2019)"
      ],
      "metadata": {
        "id": "hV1SkJcwYO1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary of lists"
      ],
      "metadata": {
        "id": "el1cqsEGYPmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dictionary of lists with new data\n",
        "avocados_dict = {\n",
        "  \"date\": [\"2019-11-17\",\"2019-12-01\"],\n",
        "  \"small_sold\": [10859987, 9291631],\n",
        "  \"large_sold\": [7674135,6238096]\n",
        "}\n",
        "\n",
        "# Convert dictionary into DataFrame\n",
        "avocados_2019 = pd.DataFrame(avocados_dict)\n",
        "\n",
        "# Print the new DataFrame\n",
        "print(avocados_2019)"
      ],
      "metadata": {
        "id": "zBeu70VnYT4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **READING AND WRITING CSVS**\n",
        "\n",
        "**CSV**\n",
        "- comma-separated values\n",
        "- common data storage file type\n",
        "- store tabular data, just like a pandas DataFrame\n",
        ""
      ],
      "metadata": {
        "id": "SfSozsO0YaB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CSV to DataFrame"
      ],
      "metadata": {
        "id": "sS5-4TPsbUO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# From previous steps\n",
        "airline_bumping = pd.read_csv(\"airline_bumping.csv\")\n",
        "print(airline_bumping.head())\n",
        "airline_totals = airline_bumping.groupby(\"airline\")[[\"nb_bumped\", \"total_passengers\"]].sum()\n",
        "airline_totals[\"bumps_per_10k\"] = airline_totals[\"nb_bumped\"] / airline_totals[\"total_passengers\"] * 10000\n",
        "\n",
        "# Print airline_totals\n",
        "print(airline_totals)"
      ],
      "metadata": {
        "id": "6eH002-zbWH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DataFrame to CSV"
      ],
      "metadata": {
        "id": "WV8P6SkrbXlp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create airline_totals_sorted\n",
        "airline_totals_sorted = airline_totals.sort_values('bumps_per_10k', ascending=False)\n",
        "\n",
        "# Print airline_totals_sorted\n",
        "print(airline_totals_sorted)\n",
        "\n",
        "# Save as airline_totals_sorted.csv\n",
        "airline_totals_sorted.to_csv('airline_totals_sorted.csv')"
      ],
      "metadata": {
        "id": "YbKcHEDCbaeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suggested More to Learn\n",
        "\n",
        "\n",
        "*   Joining Data with pandas\n",
        "*   Streamlined Data ingestion with pandas\n",
        "\n"
      ],
      "metadata": {
        "id": "s-lSMtv5bkJC"
      }
    }
  ]
}